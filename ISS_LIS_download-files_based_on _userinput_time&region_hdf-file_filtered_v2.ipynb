{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4b4763",
   "metadata": {},
   "source": [
    "### ISS-LIS_download files_based on user input of time & region of interest_hdffile filtered_v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a6cb9",
   "metadata": {},
   "source": [
    "**Earthdata API is used to access and query NASA data, for details: https://pypi.org/project/earthdata/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed050b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthdata\n",
    "from earthdata import Auth, Store, DataCollections, DataGranules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80007635",
   "metadata": {},
   "source": [
    "**To access the cell below one should have to create a .netrc file in home directory, please follow the given steps below:-**\n",
    "\n",
    "BY CMD IT, COULD BE DONE AS FOLLOWS: \n",
    "(1) cd ~ or cd $HOME,\n",
    "(2) touch .netrc,\n",
    "(3) echo \"machine urs.earthdata.nasa.gov login <uid> password <password>\" >> .netrc \n",
    "(Note that here <uid> is your user name and <password> is your password used for Earthdata search Login, enter both without the brackets),\n",
    "(4) chmod 0600 .netrc (so only you can access it)\n",
    "\n",
    "OR\n",
    "\n",
    "BY MANUAL PROCESS, IT COULD BE DONE AS FOLLOWS: \n",
    "(1) Go to dir\n",
    "(find your dir: On Windows, this might look like 'C:\\Users\\<your name>\\', on a Mac '/opt/'),\n",
    "(2) Open notepad, and paste the content in the brackets \"machine urs.earthdata.nasa.gov login <uid> password <password>\" \n",
    "(Note that here <uid> is your user name and <password> is your password used for Earthdata search Login, enter both without the brackets),\n",
    "(3) Save the notepad as '.netric'\n",
    "\n",
    "**Finally once the netric file is successfully created in your dir, you would be able to RUN the next cell and it would show the output as \"You're now authenticated with NASA Earthdata Login, True\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3502546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're now authenticated with NASA Earthdata Login\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "### AUTHENTICATION STEP\n",
    "\n",
    "auth = Auth()\n",
    "auth.login(strategy=\"netrc\")\n",
    "# are we authenticated?\n",
    "print(auth.authenticated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf6f0e",
   "metadata": {},
   "source": [
    "**Note here we need NASA authentication to access the files directly from GHRC server*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74238214",
   "metadata": {},
   "source": [
    "**We can search for collections using a pythonic API client for CMR**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "813bf527",
   "metadata": {},
   "source": [
    "###Locate DAAC (in oue case it is GHRC-DAAC to access LIS files)###\n",
    "#Query = DataCollections().daac(\"GHRCDAAC\")\n",
    "\n",
    "###Find collections in the mentioned DAAC###\n",
    "#print(f'Collections found: {Query.hits()}')\n",
    "#collections = Query.fields(['ShortName']).get(10)\n",
    "\n",
    "###Printing collection 6 of our interest which has ISS science data###\n",
    "#collections[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449bc34",
   "metadata": {},
   "source": [
    "## Please enter the date from 2017-03-01 onwards till date\n",
    "\n",
    "### Input start time of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48570c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter start time in format: YYYY-MM-DD \n",
      "2021-07-25\n"
     ]
    }
   ],
   "source": [
    "start_time = input('Enter start time in format: YYYY-MM-DD \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238dca5",
   "metadata": {},
   "source": [
    "### Input end time of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f131c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter end time in format: YYYY-MM-DD \n",
      "2021-07-30\n"
     ]
    }
   ],
   "source": [
    "end_time = input('Enter end time in format: YYYY-MM-DD \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044eb76d",
   "metadata": {},
   "source": [
    "**LET now find Non-Quality Controlled Lightning Imaging Sensor (LIS) on International Space Station (ISS) Science Data V2,  \n",
    "LIS granules for given dates and access their metadata using earthdata API get() method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbea3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We build our query, note as ISS_LIS data comes 1file per orbit so we can temporally query the data### \n",
    "### direct spatial query not possible in case of ISS_LIS data###\n",
    "### The short name for collection was found from cell[2]###\n",
    "\n",
    "from pprint import pprint\n",
    "Query = DataGranules().short_name('isslis_v2_nqc').temporal(start_time,end_time)\n",
    "\n",
    "###We get all metadata records###\n",
    "granules = Query.get()\n",
    "\n",
    "###Please uncomment to display granules\n",
    "#print(granules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2be5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Access bounding rectangle coordinates from each granules (as required to indirectly query ISS_LIS data wrt region of interest)\n",
    "### Note here these coordinates are ones when stellite is changing its orbit or doing some*\n",
    "### *maintainance/instrumentation health check\n",
    "\n",
    "space = []\n",
    "\n",
    "for i in range(len(granules)):\n",
    "    space_iss = granules[i]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['BoundingRectangles']\n",
    "    \n",
    "    ### to remove square brackets from start and end of each string\n",
    "    space_iss_1 = str(space_iss)[1:-1]\n",
    "    \n",
    "    # change datatype from string to dict \n",
    "    space_iss_1 = eval(space_iss_1)\n",
    "    space.append(space_iss_1)\n",
    "    \n",
    "###Please uncomment to display bounding rectangle coordinates\n",
    "#space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a6b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Change the dataframe from dict to dataframe\n",
    "\n",
    "import pandas as pd\n",
    "spatial = pd.DataFrame(space)\n",
    "\n",
    "###Please uncomment to display table of bounding rectangle coordinates \n",
    "#spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f77d3",
   "metadata": {},
   "source": [
    "**Now lets try to extract data URLS from the metadata of each datasets of our interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70657b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links = [granule.data_links() for granule in granules]\n",
    "\n",
    "###Please uncomment to display links\n",
    "#print(data_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e637fd8",
   "metadata": {},
   "source": [
    "**OOPS!! we are not able to access the GHRC DAAC server to directly download the files using get() method from earthdata library so lets take a long cut (atleast for time being)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a855c",
   "metadata": {},
   "source": [
    "**'https' url for GHRC-DAAC server are sorted (from 's3' ie AWS links) and than the bounding rectangle coordinates dataframe is merged with 'https' sorted dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba3924f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WestBoundingCoordinate</th>\n",
       "      <th>EastBoundingCoordinate</th>\n",
       "      <th>NorthBoundingCoordinate</th>\n",
       "      <th>SouthBoundingCoordinate</th>\n",
       "      <th>https</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145.419</td>\n",
       "      <td>119.246</td>\n",
       "      <td>51.735</td>\n",
       "      <td>-51.926</td>\n",
       "      <td>https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145.419</td>\n",
       "      <td>119.246</td>\n",
       "      <td>51.735</td>\n",
       "      <td>-51.926</td>\n",
       "      <td>https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122.839</td>\n",
       "      <td>92.982</td>\n",
       "      <td>51.741</td>\n",
       "      <td>-51.883</td>\n",
       "      <td>https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122.839</td>\n",
       "      <td>92.982</td>\n",
       "      <td>51.741</td>\n",
       "      <td>-51.883</td>\n",
       "      <td>https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.576</td>\n",
       "      <td>69.409</td>\n",
       "      <td>51.734</td>\n",
       "      <td>-51.926</td>\n",
       "      <td>https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WestBoundingCoordinate  EastBoundingCoordinate  NorthBoundingCoordinate  \\\n",
       "0                 145.419                 119.246                   51.735   \n",
       "1                 145.419                 119.246                   51.735   \n",
       "2                 122.839                  92.982                   51.741   \n",
       "3                 122.839                  92.982                   51.741   \n",
       "4                  96.576                  69.409                   51.734   \n",
       "\n",
       "   SouthBoundingCoordinate                                              https  \n",
       "0                  -51.926  https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...  \n",
       "1                  -51.926  https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...  \n",
       "2                  -51.883  https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...  \n",
       "3                  -51.883  https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...  \n",
       "4                  -51.926  https://data.ghrc.earthdata.nasa.gov/ghrcw-pro...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###covert list to dataframe .. a long cut again :)\n",
    "import pandas as pd\n",
    "url = pd.DataFrame(data_links, columns = ['https' , 'sf3'])\n",
    "\n",
    "###converted dataframe back to list sorting hhtps urls\n",
    "url_https = url['https']#.values.tolist()\n",
    "#print(inprem)\n",
    "type(url_https)\n",
    "\n",
    "### merge the bounding rectangle coordinates dataframe is merged with 'https' dorted dataframe\n",
    "result_1 = pd.concat([spatial, url_https], axis=1, join=\"outer\")\n",
    "\n",
    "result_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2917fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### An indirect method to sptially query ISS_LIS data wrt region of interest\n",
    "### Finally we filter the iss_lis data based on the condition that whenever the bounding coordinates are positive (ie longitude is between 0 to 180degrees) all the satellite orbit are covering canada region\n",
    "\n",
    "import pandas as pd\n",
    "result = result_1[(result_1['WestBoundingCoordinate'] > 0) & (result_1['EastBoundingCoordinate'] > 0)]\n",
    "\n",
    "### sorted coordinates\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21e54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let's filter list URLs with extension '.hdf' \n",
    "### NOTE that here we sort HDF files from list of HDF and NETCDF4 files as generally HDF file size are smaller than NETCDF4 files\n",
    "### Using List comprehension method (filtering list of strings based on the substring list)\n",
    "\n",
    "import re\n",
    " \n",
    "### convert dataframe to back to list dtype    \n",
    "download_https = result[\"https\"].values.tolist()\n",
    "\n",
    "def Filter(download_https, substr):\n",
    "    return [str for str in download_https if\n",
    "             any(sub in str for sub in substr)]\n",
    "     \n",
    "substr = ['.hdf']\n",
    "\n",
    "download_https_hdf = Filter(download_https, substr)\n",
    "\n",
    "### Please uncomment to display sorted .hdf links\n",
    "#download_https_hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bdbed",
   "metadata": {},
   "source": [
    "***Finally now, https urls for GHRC-DAAC server are opened using loop and temporarily sorted files are downloaded ;)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838e52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Works https url opens and data is downloaded to downloads as default location\n",
    "#import time\n",
    "import webbrowser\n",
    "\n",
    "for i in download_https_hdf:\n",
    "    reponse = webbrowser.open(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fe96d",
   "metadata": {},
   "source": [
    "***THE CODE ENDs HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e4686",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfb589",
   "metadata": {},
   "source": [
    "*Management of huge sized data files from NASA is a big challenge and so in this code not only helps to get connected with NASA server using API to get ISS_LIS data files but also further enables to query the data wrt time , bounding rectangle coordinates according to ones research interest and file type (as hdf file size is relatively smaller)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39313b2",
   "metadata": {},
   "source": [
    "**ALL THE FILTERING EFFORTS ARE JUST DONE TO OPTIMALLY USE AVAILABLE SPACE IN COMPUTER***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae99f9e",
   "metadata": {},
   "source": [
    "***AS SEEN BELOW, a significant reduction in number of datefiles to be downloaded can be clearly observed ~1/5th wrt the initial total datafiles***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "082ee91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Total number of data files after inputing time period of interest \n",
    "len(granules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b7f377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Number of data files would be reduced due to orbital sorting (orbits passing over region of interest:Canada in our case are sorted) \n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f921ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Finally further reduction in the number of datafiles is achieved by sorting hdf files from netcdf files\n",
    "len(download_https_hdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f79768",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c176a",
   "metadata": {},
   "source": [
    "**Issues yet to be addressed:**\n",
    "1) Not able to access the files from local DAAC server directly using get() method\n",
    "(By default the AWS links gets accessed and no file is downloaded)\n",
    "\n",
    "2) Have used URL open method to download files but all files are saved in downloads: How to change the directory ?\n",
    "\n",
    "3) It can be seen in the whole code that the datatype is changed from dict to list to pandas dataFrame and this can help to shorten the length of the overall code\n",
    "\n",
    "**All your inputs to further optimize the pythonic code are would be of great help and welcomed !!**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d12b575a",
   "metadata": {},
   "source": [
    "#NOT WORKING: General method to download data by earthdata API but no data is downloaded from s3 links for ISS_LIS\n",
    "\n",
    "(\n",
    "a quick try; please check open the below links of a same given dataset and check if you are able to download the data using s3 link:\n",
    "https://data.ghrc.earthdata.nasa.gov/ghrcw-protected/isslis_v2_nqc__2/202105/ISS_LIS_SC_V2.1_20210501_223343_NQC.nc\n",
    "s3://ghrcw-protected/isslis_v2_nqc__2/202105/ISS_LIS_SC_V2.1_20210501_223343_NQC.nc\n",
    ")\n",
    "\n",
    "\n",
    "#data_links = [granule.data_links(access=\"direct\") for granule in granules]\n",
    "##or if the data is an on-prem dataset\n",
    "#data_links = [granule.data_links(access=\"onprem\") for granule in granules]\n",
    "\n",
    "## The Store class allows to get the granules from on-prem locations with get()\n",
    "## NOTE: Some datasets require users to accept a Licence Agreement before accessing them\n",
    "\n",
    "#store = Store(auth)\n",
    "#files = store.get(granules, local_path=\"./data/\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e840706",
   "metadata": {},
   "source": [
    "NOT WORKING: Download URL method not working\n",
    "\n",
    "#import requests\n",
    "#response = requests.get('https://data.ghrc.earthdata.nasa.gov/ghrcw-protected/isslis_v2_nqc__2/202002/ISS_LIS_SC_V2.1_20200201_225844_NQC.hdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEODACenv",
   "language": "python",
   "name": "geodacenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
