{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# netCDF4\n\n* Opening and creating netCDF\n* Groups\n* Variables\n* Writing and retrieving data\n* Attributes\n* Dimensions\n* Multi-file datasets\n* Variable length datasets\n* Strings\n* Time coordinates\n\n\n## Opening and creating netCDF\nTo create a netCDF file from Python, you call the `Dataset` constructor. This is also the method used to open an existing netCDF file. If the file is open for write access (`mode=w`, `r+` or `a`), you may write any type of data including new dimensions, groups, variables and attributes.\n\nWhen creating a new file, the format may be specified using the format keyword in the `Dataset` constructor. The default format is NETCDF4. To see how a given file is formatted, you can examine the `data_model` attribute. Closing the netCDF file is accomplished via the `Dataset.close` method of the `Dataset` instance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from netCDF4 import Dataset\n\nrootgrp = Dataset(\"test.nc\", \"w\", format=\"NETCDF4\")\nprint(rootgrp.data_model)\n\nrootgrp.close()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Groups\nTo create Group instances, use the `Dataset.createGroup` method of a `Dataset` or `Group` instance.`Dataset.createGroup` takes a single argument, a Python string containing the name of the new group. The new `Group` instances contained within the root group can be accessed by name using the groups dictionary attribute of the `Dataset` instance. \n\nTo simplify the creation of nested groups, you can use a unix-like path as an argument to `Dataset.createGroup`. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "rootgrp = Dataset(\"test.nc\", \"w\")\nfcstgrp = rootgrp.createGroup(\"forecasts\")\nanalgrp = rootgrp.createGroup(\"analyses\")\nprint(rootgrp.groups)\n\nfcstgrp1 = rootgrp.createGroup(\"/forecasts/model1\")\nfcstgrp2 = rootgrp.createGroup(\"/forecasts/model2\")\n\nprint(\"\\n\", fcstgrp.groups)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Here's an example that shows how to navigate all the groups in a `Dataset`. The function `walktree` is a Python generator that is used to walk the directory tree. Note that printing the `Dataset` or `Group` object yields summary information about its contents.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def walktree(top):\n    yield top.groups.values()\n    for value in top.groups.values():\n        yield from walktree(value)\n\nprint('\\n', rootgrp, '\\n')\n\nfor children in walktree(rootgrp):\n    for child in children:\n        print(child)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "The code above should print:\n\n<class 'netCDF4._netCDF4.Dataset'>\nroot group (NETCDF4 data model, file format HDF5):\n    dimensions(sizes): \n    variables(dimensions): \n    groups: forecasts, analyses\n\n<class 'netCDF4._netCDF4.Group'>\ngroup /forecasts:\n    dimensions(sizes): \n    variables(dimensions): \n    groups: model1, model2\n<class 'netCDF4._netCDF4.Group'>\ngroup /analyses:\n    dimensions(sizes): \n    variables(dimensions): \n    groups: \n<class 'netCDF4._netCDF4.Group'>\ngroup /forecasts/model1:\n    dimensions(sizes): \n    variables(dimensions): \n    groups: \n<class 'netCDF4._netCDF4.Group'>\ngroup /forecasts/model2:\n    dimensions(sizes): \n    variables(dimensions): \n    groups:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Variables\nTo create a netCDF variable, use the `Dataset.createVariable` method of a `Dataset` or `Group` instance. The `Dataset.createVariable` method has two mandatory arguments, the variable name (a Python string), and the variable datatype. The variable's dimensions are given by a tuple containing the dimension names (defined previously with `Dataset.createDimension`). To create a scalar variable, simply leave out the dimensions keyword.\n\nValid datatype specifiers include: \n- `f4` (32-bit floating point), \n- `f8` (64-bit floating point), \n- `i4` (32-bit signed integer), \n- `i2` (16-bit signed integer), \n- `i8` (64-bit signed integer), \n- `i1` (8-bit signed integer), \n- `u1` (8-bit unsigned integer), \n- `u2` (16-bit unsigned integer), \n- `u4` (32-bit unsigned integer), \n- `u8` (64-bit unsigned integer), \n- or `S1` (single-character string). ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# we have to first make dimensions to create variables (more on this later)\nlevel = rootgrp.createDimension(\"level\", None)\ntime = rootgrp.createDimension(\"time\", None)\nlat = rootgrp.createDimension(\"lat\", 73)\nlon = rootgrp.createDimension(\"lon\", 144)\n\n# one dimensional data\ntimes = rootgrp.createVariable(\"time\",\"f8\",(\"time\",))\nlevels = rootgrp.createVariable(\"level\",\"i4\",(\"level\",))\nlatitudes = rootgrp.createVariable(\"lat\",\"f4\",(\"lat\",))\nlongitudes = rootgrp.createVariable(\"lon\",\"f4\",(\"lon\",))\n\n# two dimensions unlimited\ntemp = rootgrp.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))\ntemp.units = \"K\"",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Summary info on a variable instance:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(temp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Using a path to create a variable within a hierarchy of groups (intermediate groups will be made):",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ftemp = rootgrp.createVariable(\"/forecasts/model1/temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",)) ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Querying a dataset or group instance:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(rootgrp[\"/forecasts/model1\"])  # a Group instance\n\nprint(rootgrp[\"/forecasts/model1/temp\"])  # a Variable instance",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The variables in the `Dataset` or `Group` are stored in a Python dictionary:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(rootgrp.variables)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Writing and retrieving data\nSimple data assignment:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nlats =  np.arange(-90,91,2.5)\nlons =  np.arange(-180,180,2.5)\nlatitudes[:] = lats\nlongitudes[:] = lons\nprint(\"latitudes =\\n{}\".format(latitudes[:]))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Unlike NumPy's array objects, netCDF Variable objects with unlimited dimensions will grow along those dimensions if you assign data outside the currently defined range of indices.\nNote that the size of the levels variable grows when data is appended along the level dimension of the variable temp, even though no data has yet been assigned to levels.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# append along two unlimited dimensions by assigning to slice.\nnlats = len(rootgrp.dimensions[\"lat\"])\nnlons = len(rootgrp.dimensions[\"lon\"])\nprint(\"temp shape before adding data = {}\".format(temp.shape))\n\nfrom numpy.random import uniform\ntemp[0:5, 0:10, :, :] = uniform(size=(5, 10, nlats, nlons))\nprint(\"temp shape after adding data = {}\".format(temp.shape))\n\n# levels have grown, but no values yet assigned.\nprint(\"levels shape after adding pressure data = {}\".format(levels.shape))\n\n# now, assign data to levels dimension variable.\nlevels[:] =  [1000.,850.,700.,500.,300.,250.,200.,150.,100.,50.]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Attributes\nThere are two types of attributes in a netCDF file, global and variable. Global attributes provide information about a group, or the entire dataset, as a whole. Variable attributes provide information about one of the variables in a group. Global attributes are set by assigning values to `Dataset` or `Group` instance variables. Variable attributes are set by assigning values to `Variable` instances variables. Attributes can be strings, numbers or sequences. Returning to our example,",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import time\nrootgrp.description = \"an example script\"\nrootgrp.history = \"Created \" + time.ctime(time.time())\nrootgrp.source = \"netCDF4 python module tutorial\"\nlatitudes.units = \"degrees north\"\nlongitudes.units = \"degrees east\"\nlevels.units = \"hPa\"\ntemp.units = \"K\"\ntimes.units = \"hours since 0001-01-01 00:00:00.0\"\ntimes.calendar = \"gregorian\"",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The `Dataset.ncattrs` method of a `Dataset`, `Group` or `Variable` instance can be used to retrieve the names of all the netCDF attributes. This method is provided as a convenience, since using the built-in dir Python function will return a bunch of private methods and attributes that cannot (or should not) be modified by the user.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for name in rootgrp.ncattrs():\n    print(\"Global attr {} = {}\".format(name, getattr(rootgrp, name)))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The `__dict__` attribute of a `Dataset`, `Group` or `Variable` instance provides all the netCDF attribute name/value pairs in a Python dictionary:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(rootgrp.__dict__)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Attributes can be deleted from a netCDF `Dataset`, `Group` or `Variable` using the Python del statement (i.e. del grp.foo removes the attribute foo the the group grp).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Dimensions\nA dimension is created using the `Dataset.createDimension` method of a `Dataset` or `Group` instance. We used this earlier as it has to be defined before creating a variable. ",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "level = rootgrp.createDimension(\"level\", None)\ntime = rootgrp.createDimension(\"time\", None)\nlat = rootgrp.createDimension(\"lat\", 73)\nlon = rootgrp.createDimension(\"lon\", 144)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A Python string is used to set the name of the dimension, and an integer value is used to set the size. To create an unlimited dimension (a dimension that can be appended to), the size value is set to `None` or `0`. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(rootgrp.dimensions)\n\nfor dimobj in rootgrp.dimensions.values():\n    print(dimobj)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": ">>> len(lon)\n144\n>>> lon.isunlimited()\nFalse\n>>> time.isunlimited()\nTrue",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Multi-file datasets\nYou can use the `MFDataset` class to read the data as if it were contained in a single file. Instead of using a single filename to create a `Dataset` instance, create a `MFDataset` instance with either a list of filenames, or a string with a wildcard (which is then converted to a sorted list of files using the Python glob module). Variables in the list of files that share the same unlimited dimension are aggregated together, and can be sliced across multiple files. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for nf in range(10):\n    with Dataset(\"mftest%s.nc\" % nf, \"w\", format=\"NETCDF4_CLASSIC\") as f:\n        _ = f.createDimension(\"x\",None)\n        x = f.createVariable(\"x\",\"i\",(\"x\",))\n        x[0:10] = np.arange(nf*10,10*(nf+1))\n\n# Now read all the files back in at once with MFDataset\n\nfrom netCDF4 import MFDataset\nf = MFDataset(\"mftest*nc\")\nprint(f.variables[\"x\"][:])\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n 96 97 98 99]\n\n# Note that MFDataset can only be used to read, not write, multi-file datasets.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### `MFDataset(files, check=False, aggdim=None, exclude=\\[], master_file=None)`\n\n`__init__(self, files, check=False, aggdim=None, exclude=\\[], master_file=None)`\n\nOpen a Dataset spanning multiple files, making it look as if it was a single file. Variables in the list of files that share the same dimension (specified with the keyword `aggdim`) are aggregated. If `aggdim` is not specified, the unlimited is aggregated. Currently, `aggdim` must be the leftmost (slowest varying) dimension of each of the variables to be aggregated.\n\n`files`: either a sequence of netCDF files or a string with a wildcard (converted to a sorted list of files using glob) If the `master_file` kwarg is not specified, the first file in the list will become the \"master\" file, defining all the variables with an aggregation dimension which may span subsequent files. Attribute access returns attributes only from \"master\" file. The files are always opened in read-only mode.\n\n`check`: True if you want to do consistency checking to ensure the correct variables structure for all of the netcdf files. Checking makes the initialization of the `MFDataset` instance much slower. Default is False.\n\n`aggdim`: The name of the dimension to aggregate over (must be the leftmost dimension of each of the variables to be aggregated). If None (default), aggregate over the unlimited dimension.\n\n`exclude`: A list of variable names to exclude from aggregation. Default is an empty list.\n\n`master_file`: file to use as \"master file\", defining all the variables with an aggregation dimension and all global attributes.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Variable length Datasets\nNetCDF 4 has support for variable-length or \"ragged\" arrays. These are arrays of variable length sequences having the same type. To create a variable-length data type, use the `Dataset.createVLType` method of a `Dataset` or `Group` instance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "f = Dataset(\"tst_vlen.nc\",\"w\")\nvlen_t = f.createVLType(np.int32, \"phony_vlen\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The numpy datatype of the variable-length sequences and the name of the new datatype must be specified. Any of the primitive datatypes can be used (signed and unsigned integers, 32 and 64 bit floats, and characters), but compound data types cannot. A new variable can then be created using this datatype.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "x = f.createDimension(\"x\",3)\ny = f.createDimension(\"y\",4)\nvlvar = f.createVariable(\"phony_vlen_var\", vlen_t, (\"y\",\"x\"))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Since there is no native vlen datatype in NumPy, vlen arrays are represented in Python as object arrays (arrays of dtype object). These are arrays whose elements are Python object pointers, and can contain any type of python object. For this application, they must contain 1-D numpy arrays all of the same type but of varying length. In this case, they contain 1-D NumPy int32 arrays of random length between 1 and 10.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\nrandom.seed(54321)\ndata = np.empty(len(y)*len(x),object)\nfor n in range(len(y)*len(x)):\n    data[n] = np.arange(random.randint(1,10),dtype=\"int32\")+1\ndata = np.reshape(data,(len(y),len(x)))\nvlvar[:] = data\nprint(\"vlen variable =\\n{}\".format(vlvar[:]))\n\nprint('\\n', f)\n\nprint('\\n', f.variables[\"phony_vlen_var\"])\n\nprint('\\n', f.vltypes[\"phony_vlen\"])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Numpy object arrays containing Python strings can also be written as vlen variables, For vlen strings, you don't need to create a vlen data type. Instead, simply use the Python str builtin (or a numpy string datatype with fixed length greater than 1) when calling the `Dataset.createVariable` method.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "z = f.createDimension(\"z\",10)\nstrvar = f.createVariable(\"strvar\", str, \"z\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "In this example, an object array is filled with random Python strings with random lengths between 2 and 12 characters, and the data in the object array is assigned to the vlen string variable.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "chars = \"1234567890aabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\ndata = np.empty(10,\"O\")\nfor n in range(10):\n    stringlen = random.randint(2,12)\n    data[n] = \"\".join([random.choice(chars) for i in range(stringlen)])\nstrvar[:] = data\nprint(\"variable-length string variable:\\n{}\".format(strvar[:]))\n\nprint('\\n', f)\n\nprint(f.variables[\"strvar\"])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "It is also possible to set contents of vlen string variables with NumPy arrays of any string or unicode data type. Note, however, that accessing the contents of such variables will always return NumPy arrays with dtype object.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Strings\nThe most flexible way to store arrays of strings is with the Variable-length (vlen) string data type. However, this requires the use of the NETCDF4 data model, and the vlen type does not map very well as NumPy arrays (you have to use NumPy arrays of `dtype=object`, which are arrays of arbitrary Python objects). NumPy does have a fixed-width string array data type, but unfortunately the netCDF data model does not. Instead fixed-width byte strings are typically stored as arrays of 8-bit characters. To perform the conversion to and from character arrays to fixed-width NumPy string arrays, the following convention is followed by the Python interface. \n- If the `_Encoding` special attribute is set for a character array (dtype S1) variable, the chartostring utility function is used to convert the array of characters to an array of strings with one less dimension (the last dimension is interpreted as the length of each string) when reading the data. The character set (usually ascii) is specified by the `_Encoding` attribute. \n- If `_Encoding` is 'none' or 'bytes', then the character array is converted to a NumPy fixed-width byte string array (dtype S#), otherwise a NumPy unicode (dtype U#) array is created. When writing the data, stringtochar is used to convert the numpy string array to an array of characters with one more dimension. For example,",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from netCDF4 import stringtochar\nnc = Dataset('stringtest.nc','w',format='NETCDF4_CLASSIC')\n_ = nc.createDimension('nchars',3)\n_ = nc.createDimension('nstrings',None)\nv = nc.createVariable('strings','S1',('nstrings','nchars'))\ndatain = np.array(['foo','bar'],dtype='S3')\nv[:] = stringtochar(datain)           # manual conversion to char array\nprint(v[:])                           # data returned as char array\n\nv._Encoding = 'ascii'                 # this enables automatic conversion\nv[:] = datain                         # conversion to char array done internally\nprint(v[:])                           # data returned in numpy string array\n\nnc.close()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Even if the `_Encoding` attribute is set, the automatic conversion of char arrays to/from string arrays can be disabled with `Variable.set_auto_chartostring`.\n\nA similar situation is often encountered with NumPy structured arrays with subdtypes containing fixed-wdith byte strings (dtype=S#). Since there is no native fixed-length string netCDF datatype, these NumPy structure arrays are mapped onto netCDF compound types with character array elements. In this case the string <-> char array conversion is handled automatically (without the need to set the `_Encoding` attribute) using NumPy views. The structured array dtype (including the string elements) can even be used to define the compound data type - the string dtype will be converted to character array dtype under the hood when creating the netcdf compound type. Here's an example:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "nc = Dataset('compoundstring_example.nc','w')\ndtype = np.dtype([('observation', 'f4'),\n                     ('station_name','S10')])\nstation_data_t = nc.createCompoundType(dtype,'station_data')\n_ = nc.createDimension('station',None)\nstatdat = nc.createVariable('station_obs', station_data_t, ('station',))\ndata = np.empty(2,dtype)\ndata['observation'][:] = (123.,3.14)\ndata['station_name'][:] = ('Boulder','New York')\nprint('\\n', statdat.dtype)                   # strings actually stored as character arrays\n\nstatdat[:] = data                            # strings converted to character arrays internally\nprint('\\n', statdat[:])                      # character arrays converted back to strings\n\nprint('\\n', statdat[:].dtype)\n\nstatdat.set_auto_chartostring(False)         # turn off auto-conversion\nstatdat[:] = data.view(dtype=[('observation', 'f4'),('station_name','S1',10)])\nprint('\\n', statdat[:])                      # now structured array with char array subtype is returned\n\nnc.close()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Note that there is currently no support for mapping NumPy structured arrays with unicode elements (dtype U#) onto netCDF compound types, nor is there support for netCDF compound types with vlen string components.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Time Coordinates\n The functions `num2date` and `date2num` are provided by `cftime` to convert values of time to and from calender dates. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import netCDF4\nfrom netCDF4 import Dataset\nimport numpy as np\n\n# Setting up a file\nrootgrp = Dataset(\"test.nc\", \"w\")\nfcstgrp = rootgrp.createGroup(\"forecasts\")\nanalgrp = rootgrp.createGroup(\"analyses\")\n\nfcstgrp1 = rootgrp.createGroup(\"/forecasts/model1\")\nfcstgrp2 = rootgrp.createGroup(\"/forecasts/model2\")\n\n# we have to first make dimensions to create variables (more on this later)\nlevel = rootgrp.createDimension(\"level\", None)\ntime = rootgrp.createDimension(\"time\", None)\nlat = rootgrp.createDimension(\"lat\", 73)\nlon = rootgrp.createDimension(\"lon\", 144)\n\n# one dimensional data\ntimes = rootgrp.createVariable(\"time\",\"f8\",(\"time\",))\nlevels = rootgrp.createVariable(\"level\",\"i4\",(\"level\",))\nlatitudes = rootgrp.createVariable(\"lat\",\"f4\",(\"lat\",))\nlongitudes = rootgrp.createVariable(\"lon\",\"f4\",(\"lon\",))\n\n# two dimensions unlimited\ntemp = rootgrp.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))\ntemp.units = \"K\"\n\n# creating attributes\nimport time\nrootgrp.description = \"an example script\"\nrootgrp.history = \"Created \" + time.ctime(time.time())\nrootgrp.source = \"netCDF4 python module tutorial\"\nlatitudes.units = \"degrees north\"\nlongitudes.units = \"degrees east\"\nlevels.units = \"hPa\"\ntemp.units = \"K\"\ntimes.units = \"hours since 0001-01-01 00:00:00.0\"\ntimes.calendar = \"gregorian\"\n\n# Setting up values\nimport numpy as np\nlats =  np.arange(-90,91,2.5)\nlons =  np.arange(-180,180,2.5)\nlatitudes[:] = lats\nlongitudes[:] = lons\nnlats = len(rootgrp.dimensions[\"lat\"])\nnlons = len(rootgrp.dimensions[\"lon\"])\n\nfrom numpy.random import uniform\ntemp[0:5, 0:10, :, :] = uniform(size=(5, 10, nlats, nlons))\nlevels[:] =  [1000.,850.,700.,500.,300.,250.,200.,150.,100.,50.]\ntemp[0, 0, [0,1,2,3], [0,1,2,3]].shape\ntempdat = temp[::2, [1,3,6], lats>0, lons>0]\n\n# fill in times.\nfrom datetime import datetime, timedelta\nfrom cftime import num2date, date2num\ndates = [datetime(2001,3,1)+n*timedelta(hours=12) for n in range(temp.shape[0])]\ntimes[:] = date2num(dates,units=times.units,calendar=times.calendar)\nprint(\"time values (in units {}):\\n{}\".format(times.units, times[:]))\n\ndates = num2date(times[:],units=times.units,calendar=times.calendar)\nprint(\"dates corresponding to time values:\\n{}\".format(dates))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "`num2date` converts numeric values of time in the specified units and calendar to datetime objects, and `date2num` does the reverse. All the calendars currently defined in the CF metadata convention are supported. A function called `date2index` is also provided which returns the indices of a netCDF time variable corresponding to a sequence of datetime instances.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### `date2num(dates, units, calendar=None, has_year_zero=None)`\n\nReturn numeric time values given datetime objects. The units of the numeric time values are described by the units argument and the `calendar` keyword. The datetime objects must be in UTC with no time-zone offset. If there is a time-zone offset in units, it will be applied to the returned numeric values.\n\n`dates`: A datetime object or a sequence of datetime objects. The datetime objects should not include a time-zone offset. They can be either native Python datetime instances (which use the proleptic gregorian calendar) or `cftime.datetime` instances.\n\n`units`: a string of the form since describing the time units. can be days, hours, minutes, seconds, milliseconds or microseconds. is the time origin. months since is allowed only for the `360_day` calendar and `common_years` since is allowed only for the `365_day` calendar.\n\n`calendar`: describes the calendar to be used in the time calculations. All the values currently defined in the CF metadata convention <http://cfconventions.org/cf-conventions/cf-conventions#calendar>__ are supported. Valid calendars 'standard', 'gregorian', 'proleptic_gregorian' 'noleap', '365_day', '360_day', 'julian', 'all_leap', '366_day'. Default is `None` which means the calendar associated with the first input datetime instance will be used.\n\n`has_year_zero`: If set to True, astronomical year numbering is used and the year zero exists. If set to False for real-world calendars, then historical year numbering is used and the year 1 is preceded by year -1 and no year zero exists. The defaults are set to conform with CF version 1.9 conventions (False for 'julian', 'gregorian'/'standard', True for 'proleptic_gregorian' (ISO 8601) and True for the idealized calendars 'noleap'/'365_day', '360_day', 366_day'/'all_leap') Note that CF v1.9 does not specifically mention whether year zero is allowed in the proleptic_gregorian calendar, but ISO-8601 has a year zero so we have adopted this as the default. The defaults can only be over-ridden for the real-world calendars, for the idealized calendars the year zero always exists and the `has_year_zero` kwarg is ignored. This kwarg is not needed to define calendar systems allowed by CF (the calendar-specific defaults do this).\n\nreturns a numeric time value, or an array of numeric time values with approximately 1 microsecond accuracy.",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### `num2date(times, units, calendar=u'standard', only_use_cftime_datetimes=True, only_use_python_datetimes=False, has_year_zero=None)`\n\nReturn datetime objects given numeric time values. The units of the numeric time values are described by the units argument and the calendar keyword. The returned datetime objects represent UTC with no time-zone offset, even if the specified units contain a time-zone offset.\n\n`times`: numeric time values.\n\n`units`: a string of the form since describing the time units. can be days, hours, minutes, seconds, milliseconds or microseconds. is the time origin. months since is allowed only for the `360_day` calendar and `common_years` since is allowed only for the `365_day` calendar.\n\n`calendar`: describes the calendar used in the time calculations. All the values currently defined in the CF metadata convention <http://cfconventions.org/cf-conventions/cf-conventions#calendar>__ are supported. Valid calendars 'standard', 'gregorian', 'proleptic_gregorian' 'noleap', '365_day', '360_day', 'julian', 'all_leap', '366_day'. Default is 'standard', which is a mixed Julian/Gregorian calendar.\n\n`only_use_cftime_datetimes`: if False, Python `datetime.datetime` objects are returned from `num2date` where possible; if True dates which subclass `cftime.datetime` are returned for all calendars. Default True.\n\n`only_use_python_datetimes`: always return Python `datetime.datetime` objects and raise an error if this is not possible. Ignored unless `only_use_cftime_datetimes=False`. Default False.\n\n`has_year_zero`: if set to True, astronomical year numbering is used and the year zero exists. If set to False for real-world calendars, then historical year numbering is used and the year 1 is preceded by year -1 and no year zero exists. The defaults are set to conform with CF version 1.9 conventions (False for 'julian', 'gregorian'/'standard', True for 'proleptic_gregorian' (ISO 8601) and True for the idealized calendars 'noleap'/'365_day', '360_day', 366_day'/'all_leap') The defaults can only be over-ridden for the real-world calendars, for the the idealized calendars the year zero always exists and the `has_year_zero` kwarg is ignored. This kwarg is not needed to define calendar systems allowed by CF (the calendar-specific defaults do this).\n\nreturns a datetime instance, or an array of datetime instances with microsecond accuracy, if possible.\n\nNote: If `only_use_cftime_datetimes=False` and `use_only_python_datetimes=False`, the datetime instances returned are 'real' Python datetime objects if `calendar='proleptic_gregorian'`, or `calendar='standard'` or 'gregorian' and the date is after the breakpoint between the Julian and Gregorian calendars (1582-10-15). Otherwise, they are `ctime.datetime` objects which support some but not all the methods of native Python datetime objects. The datetime instances do not contain a time-zone offset, even if the specified units contains one.",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### `MFTime(time, units=None, calendar=None)`\n`__init__(self, time, units=None, calendar=None)`\n\nCreate a time Variable with units consistent across a multifile dataset.\n\n`time`: Time variable from a `MFDataset`.\n\n`units`: Time units, for example, 'days since 1979-01-01'. If `None`, use the units from the master variable.\n\n`calendar`: Calendar overload to use across all files, for example, 'standard' or 'gregorian'. If `None`, check that the calendar attribute is present on each variable and values are unique across files raising a `ValueError` otherwise.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Review Questions\n\nCreate a file. Create a hierarchy of groups and datasets with the following details:\n\n- These\n    - crazy\n        - dataset1 (1-100)\n    - weird\n- Many\n    - odd\n        - dataset2 (2D)\n    - wacky\n    - unusual\n        - varlendataset (a variable length dataset) \n\nPrint dataset2 using iteration.\n\nCreate a minimum of 2 attributes for each dataset.",
      "metadata": {}
    }
  ]
}