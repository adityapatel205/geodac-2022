{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h5py\n",
    "\n",
    "* Reading in a file\n",
    "    * Valid modes of this function\n",
    "    * Drivers\n",
    "    * Version\n",
    "    * Filenames\n",
    "* Closing files\n",
    "* Groups\n",
    "    * Creating groups\n",
    "    * Dictionary similarities\n",
    "* Datasets\n",
    "    * Creating datasets\n",
    "    * Reading and writing datasets\n",
    "    * Multiple indexing\n",
    "    * Length and iteration\n",
    "    * Resizable datasets\n",
    "    * Empty or null datasets\n",
    "* Attributes\n",
    "* Dimension scales\n",
    "* Strings\n",
    "* Variable length strings\n",
    "* Variable length datasets\n",
    "* Object names\n",
    "* Hardlinks\n",
    "* Softlinks\n",
    "* References\n",
    "    * Object References\n",
    "    * Region References\n",
    "    * References in a dataset\n",
    "    * Referencing in an attribute\n",
    "    * Null Referencing\n",
    "* User block\n",
    "* Opaque data\n",
    "* Single Writer Multiple Reader\n",
    "* Virtual Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a file\n",
    "To open and create files, the `File()` function is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('VNP46A2.A2022130.h05v05.001.2022138144341.h5', \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid modes of this function are:\n",
    "`r` - For reading only, the file must exist (it is also the default).\n",
    "\n",
    "`r+` - For reading and writing, the file must exist.\n",
    "\n",
    "`w` - Used to create a file or truncate if it exists.\n",
    "\n",
    "`w-` or `x` - For creating a file, the command fails if the file already exists.\n",
    "\n",
    "`a` - For reading and writing if it already exists, creates a file otherwise. \n",
    "\n",
    "### Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b6d72d641075>:3: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File('VNP46A2.A2022130.h05v05.001.2022138144341.h5', driver=None)\n"
     ]
    }
   ],
   "source": [
    "# f = h5py.File('myFile.hdf', driver=<driver name>, <driver_kwds>)\n",
    "\n",
    "f = h5py.File('VNP46A2.A2022130.h05v05.001.2022138144341.h5', driver=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 ships with a variety of different low-level drivers, which map the logical HDF5 address space to different storage mechanisms. You can specify which driver you want to use when the file is opened. \n",
    "\n",
    "`None` is the recommended option as it uses the standard HDF5 driver of the current platform. (Windows, HSFD_WINDOWS; UNIX, HSFD_SEC2) \n",
    "\n",
    "`core` allows one to store and manipulate the data in memory and optionally write it back out when the file is closed. Using this with an existing file and a reading mode will read the entire file into memory. \n",
    "\n",
    "`family` is used to store the file on disk as a series of fixed length chunks. \n",
    "\n",
    "### Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('newfile1.hdf5', \"w\", libver='earliest')    # most compatible\n",
    "f = h5py.File('newfile2.hdf5', \"w\", libver='latest')      # most modern\n",
    "f = h5py.File('newfile3.hdf5', \"w\", libver=('earliest', 'v108')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, objects will be written in the most compatible fashion. By using the `libver` option of `File()` the sophistication can be specified. For example, specifying as above `v108` for HDF5 1.8 and `v110` for HDF5 1.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Filenames\n",
    "Different operating systems (and different file systems) store filenames with different encodings. Additionally, in Python there are at least two different representations of filenames, as encoded bytes or as a Unicode string (str on Python 3).\n",
    "h5py’s high-level interfaces always return filenames as str, e.g. `File.filename`. h5py accepts filenames as either str or bytes. In most cases, using Unicode (str) paths is preferred (to be used on macOS and Windows), but there are some caveats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing files\n",
    "To close a file you can either leave a `with h5py.File(...)` block or call `File.close()`. Any groups or datasets will then be unusable. Else, if a file goes out of scope in your Python code the file will only be closed when there are no remaining objects belonging to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('VNP46A2.A2022130.h05v05.001.2022138144341.h5', 'r') as f1:\n",
    "    ds = f1['HDFEOS']\n",
    "\n",
    "#ds[0]         # ERROR - can't access dataset, because f1 is closed:\n",
    "\n",
    "def get_dataset():\n",
    "    f2 = h5py.File('VNP46A2.A2022130.h05v05.001.2022138144341.h5', 'r')\n",
    "    return f2['HDFEOS']['GRIDS']['VNP_Grid_DNB'][\"Data Fields\"]['Latest_High_Quality_Retrieval'][:]\n",
    "ds = get_dataset()\n",
    "\n",
    "ds[0]         # OK - f2 is out of scope, but the dataset reference keeps it open:\n",
    "\n",
    "\n",
    "del ds        # Now f2.h5 will be closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups\n",
    "\n",
    "Groups are the container mechanism of HDF5 files. They are a bit like keys of dictionaries with the datasets being the values. Another way to think of groups is like folders for the datasets. \n",
    "\n",
    "### Creating groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "[]\n",
      "/bar\n",
      "/bar/baz\n",
      "/some/long/path\n",
      "/some/long\n",
      "['bar', 'some']\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('createdExample.hdf5','w')\n",
    "print(f.name)\n",
    "\n",
    "print(list(f.keys()))\n",
    "\n",
    "grp = f.create_group(\"bar\")\n",
    "print(grp.name)\n",
    "\n",
    "subgrp = grp.create_group(\"baz\")\n",
    "print(subgrp.name)\n",
    "\n",
    "grp2 = f.create_group(\"/some/long/path\")\n",
    "print(grp2.name)\n",
    "\n",
    "grp3 = f['/some/long']\n",
    "print(grp3.name)\n",
    "\n",
    "print(list(f.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary similarities\n",
    "Groups have some similar methods to dictionaries including: `keys()`, `values()`, indexing syntax and support iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HDFEOS', 'HDFEOS INFORMATION']\n",
      "<HDF5 group \"/HDFEOS/GRIDS/VNP_Grid_DNB/Data Fields\" (7 members)>\n",
      "ValuesViewHDF5(<HDF5 group \"/HDFEOS/GRIDS/VNP_Grid_DNB/Data Fields\" (7 members)>)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('VNP46A2.A2022130.h05v05.001.2022138144341.h5', \"r\")\n",
    "\n",
    "print(list(f.keys()))\n",
    "\n",
    "subgrp = f['HDFEOS']['GRIDS']['VNP_Grid_DNB']['Data Fields']\n",
    "print(subgrp)\n",
    "print(subgrp.values())\n",
    "\n",
    "# missing = subgrp[\"missing\"]   # raises an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects can be deleted using the following syntax:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# please don't run this\n",
    "# del subgroup[\"MyDataset\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Similar to NumPy arrays, they are homogeneous collections of data elements with an immutable data type. They are represented by a thin proxy class which supports NumPy operations like slicing, along with a variety of descriptive attributes: shape, size, ndim, dtype, and nbytes. \n",
    "### Creating datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "<HDF5 dataset \"default\": shape (100,), type \"<f4\">\n",
      "<HDF5 dataset \"ints\": shape (100,), type \"<i8\">\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('createdExample1.hdf5','w')\n",
    "print(f.name)\n",
    "\n",
    "dset = f.create_dataset(\"default\", (100,))\n",
    "print(dset)\n",
    "dset = f.create_dataset(\"ints\", (100,), dtype='i8')\n",
    "print(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New datasets are created using either `Group.create_dataset()` or `Group.require_dataset()`. Existing datasets should be retrieved using the group indexing syntax (`dset = group[\"name\"]`).\n",
    "\n",
    "To initialise a dataset, all you have to do is specify a name, shape, and optionally the data type (defaults to 'f').\n",
    "\n",
    "You may also initialize the dataset to an existing numpy array by providing the data parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"init\": shape (100,), type \"<i8\">\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.arange(100)\n",
    "dset = f.create_dataset(\"init\", data=arr)\n",
    "print(dset)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords shape and dtype may be specified along with data; if so, they will override `data.shape` and `data.dtype`. It’s required that (1) the total number of points in shape match the total number of points in `data.shape`, and that (2) it’s possible to cast `data.dtype` to the requested dtype.\n",
    "\n",
    "### Reading and writing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset(\"MyDataset\", (10,10,10), 'f')\n",
    "print(dset[0,0,0])\n",
    "print()\n",
    "print(dset[0,2:10,1:9:3])\n",
    "print()\n",
    "print(dset[:,::2,5])\n",
    "print()\n",
    "print(dset[0])\n",
    "print()\n",
    "print(dset[1,5])\n",
    "print()\n",
    "print(dset[0,...])\n",
    "print()\n",
    "print(dset[...,6])\n",
    "print()\n",
    "print(dset[()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following NumPy slicing arguments are recognized:\n",
    "\n",
    "- Indices: anything that can be converted to a Python long\n",
    "- Slices (i.e. `[:]` or `[0:10]`)\n",
    "- Field names, in the case of compound data\n",
    "- At most one Ellipsis (...) object\n",
    "- An empty tuple (`()`) to retrieve all data or scalar data\n",
    "\n",
    "For compound data, it is advised to separate field names from the numeric slices:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> dset.fields(\"FieldA\")[:10]   # Read a single field\n",
    ">>> dset[:10][\"FieldA\"]          # Read all fields, select in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple slicing, broadcasting is supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "dset[0,:,:] = np.arange(10) \n",
    "print(dset[()])               # Broadcasts to (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple indexing\n",
    "Indexing a dataset once loads a NumPy array into memory. If you try to index it twice to write data, you may be surprised that nothing seems to have happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('my_hdf5_file.h5', 'w')\n",
    "dset = f.create_dataset(\"test\", (2, 2))\n",
    "dset[0][1] = 3.0      # No effect!\n",
    "print(dset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment above only modifies the loaded array. It’s equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "new_array = dset[0]\n",
    "new_array[1] = 3.0\n",
    "print(new_array[1])     # 3.0\n",
    "\n",
    "print(dset[0][1])       # 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write to the dataset, combine the indexes in a single step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "dset[0, 1] = 3.0\n",
    "print(dset[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length and iteration \n",
    "\n",
    "As with NumPy arrays, the `len()` of a dataset is the length of the first axis, and iterating over a dataset iterates over the first axis. However, modifications to the yielded data are not recorded in the file. Resizing a dataset while iterating has undefined results.\n",
    "\n",
    "### Resizable datasets\n",
    "\n",
    "Datasets can be resized once created up to a maximum size, by calling `Dataset.resize()`. You specify this maximum size when creating the dataset, via the keyword `maxshape`.\n",
    "Any (or all) axes may also be marked as “unlimited”, in which case they may be increased up to the HDF5 per-axis limit of 2\\*\\*64 elements. Indicate these axes using `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset(\"resizable\", (10,10), maxshape=(500, 20))\n",
    "dset = f.create_dataset(\"unlimited\", (10, 10), maxshape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty or null datasets\n",
    "These are not the same as an array with a shape of `()`, or a scalar dataspace in HDF5 terms. Instead, it is a dataset with an associated type, no data, and no shape. In h5py, we represent this as either a dataset with shape `None`, or an instance of `h5py.Empty`. Empty datasets and attributes cannot be sliced.\n",
    "\n",
    "To create an empty attribute, use `h5py.Empty`. Reading an empty attribute returns `h5py.Empty`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an empty dataset, you can define a dtype but no shape in create dataset or define data to an instance of `h5py.Empty`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "f = h5py.File('my_hdf5_file.h5', 'w')\n",
    "\n",
    "EmptyDset1 = f.create_dataset(\"EmptyDataset1\", dtype=\"f\")\n",
    "\n",
    "#EmptyDset2 = f.create_dataset(\"EmptyDataset2\", data=h5py.Empty(\"f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An empty dataset has shape defined as `None`, which is the best way of determining whether a dataset is empty or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"EmptyDataset1\": shape None, type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "print(EmptyDset1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dtype of the dataset can be accessed via `<dset>.dtype`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "They are small named pieces of data attached directly to Group and Dataset objects. This is the official way to store metadata in HDF5.\n",
    "Each Group or Dataset has a small proxy object attached to it, at `<obj>.attrs`. Attributes have the following properties:\n",
    "- They may be created from any scalar or NumPy array\n",
    "- There is no partial I/O (i.e. slicing); the entire attribute must be read.\n",
    "- The `.attrs` proxy objects are of class `AttributeManager`, below. This class supports a dictionary-style interface.\n",
    "- By default, attributes are iterated in alphanumeric order. However, if group or dataset is created with `track_order=True`, the attribute insertion order is remembered (tracked) in HDF5 file, and iteration uses that order. The latter is consistent with Python 3.7+ dictionaries. The default `track_order` for all new groups and datasets can be specified globally with `h5.get_config().track_order`.\n",
    "\n",
    "`__iter__()` Get an iterator over attribute names.\n",
    "\n",
    "`__contains__(name)` Determine if attribute name is attached to this object.\n",
    "\n",
    "`__getitem__(name)` Retrieve an attribute.\n",
    "\n",
    "`__setitem__(name, val)` Create an attribute, overwriting any existing attribute. The type and shape of the attribute are determined automatically by h5py.\n",
    "\n",
    "`__delitem__(name)` Delete an attribute. KeyError if it doesn’t exist.\n",
    "\n",
    "`keys()` Get the names of all attributes attached to this object. Returns set-like object.\n",
    "\n",
    "`values()` Get the values of all attributes attached to this object. Returns collection or bag-like object.\n",
    "\n",
    "`items()` Get (name, value) tuples for all attributes attached to this object. Returns collection or set-like object.\n",
    "\n",
    "`get(name, default=None)` Retrieve name, or default if no such attribute exists.\n",
    "\n",
    "`get_id(name)` Get the low-level AttrID for the named attribute.\n",
    "\n",
    "`create(name, data, shape=None, dtype=None)` Create a new attribute, with control over the shape and type. Any existing attribute will be overwritten.\n",
    " Parameters: \n",
    "- name (String) – Name of the new attribute\n",
    "- data – Value of the attribute; will be put through numpy.array(data).\n",
    "- shape (Tuple) – Shape of the attribute. Overrides data.shape if both are given, in which case the total number of points must be unchanged.\n",
    "- dtype (NumPy dtype) – Data type for the attribute. Overrides data.dtype if both are given.\n",
    "\n",
    "`modify(name, value)`\n",
    "Change the value of an attribute while preserving its type and shape. Unlike `AttributeManager.__setitem__()`, if the attribute already exists, only its value will be changed. This can be useful for interacting with externally generated files, where the type and shape must not be altered. If the attribute doesn’t exist, it will be created with a default shape and type. Parameters:\n",
    "- name (String) – Name of attribute to modify.\n",
    "- value – New value. Will be put through numpy.array(value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension scales\n",
    "\n",
    "HDF5 allows the dimensions of data to be labeled, for example:\n",
    "\n",
    "Note that the first dimension, which has a length of 4, has been labeled “z”, the third dimension (in this case the fastest varying dimension), has been labeled “x”, and the second dimension was given no label at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('example123.h5', 'w')\n",
    "f['data'] = np.ones((4, 3, 2), 'f')\n",
    "\n",
    "f['data'].dims[0].label = 'z'\n",
    "f['data'].dims[2].label = 'x'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use HDF5 datasets as dimension scales. For example, if we have the file below, we are going to treat the x1, x2, y1, and z1 datasets as dimension scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['x1'] = [1, 2]\n",
    "f['x2'] = [1, 1.1]\n",
    "f['y1'] = [0, 1, 2]\n",
    "f['z1'] = [0, 1, 4, 9]\n",
    "\n",
    "f['x1'].make_scale()\n",
    "f['x2'].make_scale('x2 name')\n",
    "f['y1'].make_scale('y1 name')\n",
    "f['z1'].make_scale('z1 name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a dimension scale, you may provide a name for that scale. In this case, the x1 scale was not given a name, but the others were. Now we can associate these dimension scales with the primary dataset. Note that two dimension scales were associated with the third dimension of data. You can also detach a dimension scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['data'].dims[0].attach_scale(f['z1'])\n",
    "f['data'].dims[1].attach_scale(f['y1'])\n",
    "f['data'].dims[2].attach_scale(f['x1'])\n",
    "f['data'].dims[2].attach_scale(f['x2'])\n",
    "\n",
    "f['data'].dims[2].detach_scale(f['x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, lets assume that we have both x1 and x2 still associated with the third dimension of data. You can attach a dimension scale to any number of HDF5 datasets, you can even attach it to multiple dimensions of a single HDF5 dataset. Now that the dimensions of data have been labeled, and the dimension scales for the various axes have been specified, we have provided much more context with which data can be interpreted. For example, if you want to know the labels for the various dimensions of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['z', '', 'x']\n"
     ]
    }
   ],
   "source": [
    "print([dim.label for dim in f['data'].dims])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the names of the dimension scales associated with the “x” axis, the method below works and `items()` and `values()` methods are also provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "print(f['data'].dims[2].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension scales themselves can also be accessed with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\"x\" dimension 2 of HDF5 dataset at 4602904384>\n",
      "<\"x\" dimension 2 of HDF5 dataset at 4602904384>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(f['data'].dims[2])#[1]\n",
    "\n",
    "# or:\n",
    "\n",
    "print(f['data'].dims[2])#['x2 name'])\n",
    "# if:\n",
    "print(True == f['data'].dims[2] == f['x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "though, beware that if you attempt to index the dimension scales with a string, the first dimension scale whose name matches the string is the one that will be returned. There is no guarantee that the name of the dimension scale is unique.\n",
    "\n",
    "Nested dimension scales are not permitted: if a dataset has a dimension scale attached to it, converting the dataset to a dimension scale will fail, since the HDF5 specification doesn’t allow this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> f['data'].make_scale()\n",
    "RuntimeError: Unspecified error in H5DSset_scale (return value <0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings\n",
    "\n",
    "String data in HDF5 datasets is read as bytes by default: bytes objects for variable-length strings, or numpy bytes arrays ('S' dtypes) for fixed-length strings. Use `Dataset.asstr()` to retrieve str objects.\n",
    "\n",
    "Variable-length strings in attributes are read as str objects. These are decoded as UTF-8 with surrogate escaping for unrecognised bytes.\n",
    "When creating a new dataset or attribute, Python str or bytes objects will be treated as variable-length strings, marked as UTF-8 and ASCII respectively. Numpy bytes arrays ('S' dtypes) make fixed-length strings. You can use `string_dtype()` to explicitly specify any HDF5 string datatype.\n",
    "\n",
    "When writing data to an existing dataset or attribute, data passed as bytes is written without checking the encoding. Data passed as Python str objects is encoded as either ASCII or UTF-8, based on the HDF5 datatype. In either case, null bytes ('\\x00') in the data will cause an error.\n",
    "\n",
    "NumPy also has a Unicode type, a UTF-32 fixed-width format (4-byte characters). HDF5 has no support for wide characters. Rather than trying to hack around this and “pretend” to support it, h5py will raise an error if you try to store data of this type.\n",
    "\n",
    "If you have a non-text blob in a Python byte string (as opposed to ASCII or UTF-8 encoded text, which is fine), you should wrap it in a void type for storage. This will map to the HDF5 OPAQUE datatype, and will prevent your blob from getting mangled by the string machinery.\n",
    "Here’s an example of how to store binary data in an attribute, and then recover it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset(\"test\", (5,5,5))\n",
    "\n",
    "binary_blob = b\"Hello\\x00Hello\\x00\"\n",
    "dset.attrs[\"attribute_name\"] = np.void(binary_blob)\n",
    "out = dset.attrs[\"attribute_name\"]\n",
    "binary_blob = out.tobytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable length strings\n",
    "In HDF5, data in variable length (VL) format is stored as arbitrary-length vectors of a base type. In particular, strings are stored C-style in null-terminated buffers. NumPy has no native mechanism to support this. Unfortunately, this is the standard for representing strings in the HDF5 C API, and in many HDF5 applications. \n",
    "\n",
    "Thankfully, NumPy has a generic pointer type in the form of the “object” (“O”) dtype. In h5py, variable-length strings are mapped to object arrays. A small amount of metadata attached to an “O” dtype tells h5py that its contents should be converted to VL strings when stored in the file.\n",
    "\n",
    "Existing VL strings can be read and written to with no additional effort; Python strings and fixed-length NumPy strings can be auto-converted to VL data and stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "string_info(encoding='utf-8', length=None)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('newexample1.hdf5', \"w\")\n",
    "dt = h5py.string_dtype(encoding='utf-8')\n",
    "ds = f.create_dataset('VLDS', (100,100), dtype=dt)\n",
    "print(ds.dtype.kind)\n",
    "\n",
    "print(h5py.check_string_dtype(ds.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable length data\n",
    "Starting with h5py 2.3, variable-length types are not restricted to strings. For example, you can create a “ragged” array of integers. Single elements are read as NumPy arrays and multidimensional selections produce an object array whose members are integer arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-be5a4d4c5315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlen_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vlen_int'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "dt = h5py.vlen_dtype(np.dtype('int32'))\n",
    "dset = f.create_dataset('vlen_int', (100,), dtype=dt)\n",
    "dset[0] = [1,2,3]\n",
    "dset[1] = [1,2,3,4,5]\n",
    "\n",
    "# >>> dset[0]\n",
    "# array([1, 2, 3], dtype=int32)\n",
    "\n",
    "# >>> dset[0:2]\n",
    "# array([array([1, 2, 3], dtype=int32), array([1, 2, 3, 4, 5], dtype=int32)], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that NumPy doesn’t support ragged arrays, and the ‘arrays of arrays’ h5py uses as a workaround are not as convenient or efficient as regular NumPy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object names\n",
    "Unicode strings are used exclusively for object names in the file.\n",
    "\n",
    "You can supply either byte or unicode strings when creating or retrieving objects. If a byte string is supplied, it will be used as-is; Unicode strings will be encoded as UTF-8.\n",
    "\n",
    "In the file, h5py uses the most-compatible representation; H5T_CSET_ASCII for characters in the ASCII range; H5T_CSET_UTF8 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "<HDF5 dataset \"name\": shape (5, 5, 5), type \"<f4\">\n",
      "<HDF5 dataset \"name2\": shape (2, 2, 2), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "print(f.name)\n",
    "\n",
    "grp = f.create_dataset(b\"name\", (5,5,5))\n",
    "print(grp)\n",
    "grp2 = f.create_dataset(\"name2\", (2,2,2))\n",
    "print(grp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardlinks\n",
    "When assigning an object to a name in the group, for NumPy arrays or other data the default is to create an HDF5 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"name\": shape (), type \"<i8\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File('example.hdf5','w')\n",
    "\n",
    "grp = f.create_group(\"group1\")\n",
    "\n",
    "grp[\"name\"] = 42\n",
    "out = grp[\"name\"]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the object is an existing group/dataset, a new link is made (the dataset is not copied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"other name\": shape (), type \"<i8\">\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "grp[\"other name\"] = out\n",
    "print(grp[\"other name\"])\n",
    "\n",
    "print(grp[\"other name\"] == grp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softlinks \n",
    "HDF5 groups can contain a text path instead of a pointer to the object itself. You can create these in h5py by using `h5py.SoftLink`. If the target is removed, they will dangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = h5py.File('examplename.hdf5','w')\n",
    "group = myfile.create_group(\"somegroup\")\n",
    "myfile[\"alias\"] = h5py.SoftLink('/somegroup')\n",
    "\n",
    "del myfile['somegroup']\n",
    "#print(myfile['alias'])       # this would return an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Object References\n",
    "Every high-level object in h5py has a read-only property `ref`, which when accessed returns a new object reference.\n",
    "\n",
    "“Dereferencing” these objects is straightforward; use the same syntax as when opening any other object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 object reference>\n",
      "<HDF5 group \"/some/group\" (0 members)>\n"
     ]
    }
   ],
   "source": [
    "myfile = h5py.File('myfile.hdf5', 'w')\n",
    "grp2 = myfile.create_group(\"/some/group\")\n",
    "mygroup = myfile['/some/group']\n",
    "ref = mygroup.ref\n",
    "print(ref)\n",
    "\n",
    "mygroup2 = myfile[ref]\n",
    "print(mygroup2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region References\n",
    "Region references always contain a selection. You create them using the dataset property `regionref` and standard NumPy slicing syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 region reference>\n"
     ]
    }
   ],
   "source": [
    "myds = myfile.create_dataset('dset', (200,200))\n",
    "regref = myds.regionref[0:10, 0:5]\n",
    "print(regref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference itself can now be used in place of slicing arguments to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = myds[regref]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selections which don’t conform to a regular grid, h5py copies the behavior of NumPy’s fancy indexing, which returns a 1D array. Note that for h5py release before 2.2, h5py always returns a 1D array.\n",
    "\n",
    "In addition to storing a selection, region references inherit from object references, and can be used anywhere an object reference is accepted. In this case the object they point to is the dataset used to create them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References in a dataset\n",
    "These dtypes are available from h5py for references and region references:\n",
    "\n",
    "- `h5py.ref_dtype` - for object references\n",
    "- `h5py.regionref_dtype` - for region references\n",
    "\n",
    "To store an array of references, use the appropriate dtype when creating the dataset. You can read from and write to the array as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 object reference>\n"
     ]
    }
   ],
   "source": [
    "ref_dataset = myfile.create_dataset(\"MyRefs\", (100,), dtype=h5py.ref_dtype)\n",
    "\n",
    "ref_dataset[0] = myfile.ref\n",
    "print(ref_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References in an attribute\n",
    "Simply assign the reference to a name; h5py will figure it out and store it with the correct type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "myref = myfile.ref\n",
    "myfile.attrs[\"Root group reference\"] = myref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null References\n",
    "When you create a dataset of reference type, the uninitialized elements are “null” references. H5py uses the truth value of a reference object to indicate whether or not it is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(bool(myfile.ref))\n",
    "\n",
    "nullref = ref_dataset[50]\n",
    "print(bool(nullref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Userblock\n",
    "\n",
    "HDF5 allows the user to insert arbitrary data at the beginning of the file, in a reserved space called the user block. \n",
    "The length of the user block must be specified when the file is created. It can be either zero (the default) or a power of two greater than or equal to 512. \n",
    "You can specify the size of the user block when creating a new file, via the `userblock_size` keyword to `File`; the userblock size of an open file can likewise be queried through the `File.userblock_size` property.\n",
    "Modifying the user block on an open file is not supported; this is a limitation of the HDF5 library. However, once the file is closed you are free to read and write data at the start of the file, provided your modifications don’t leave the user block region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opaque data (Date-time array)\n",
    "Numpy `datetime64` and `timedelta64` dtypes have no equivalent in HDF5 (the HDF5 time type is broken and deprecated). h5py allows you to store such data with an HDF5 opaque type; it can be read back correctly by h5py, but won’t be interoperable with other tools.\n",
    "Here’s an example of storing and reading a datetime array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'h5py' has no attribute 'opaque_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d83a5c0ca356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2019-09-22T17:38:30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmyfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopaque_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'h5py' has no attribute 'opaque_dtype'"
     ]
    }
   ],
   "source": [
    "arr = np.array([np.datetime64('2019-09-22T17:38:30')])\n",
    "myfile['data'] = arr.astype(h5py.opaque_dtype(arr.dtype))\n",
    "print(myfile['data'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `h5py.opaque_dtype(dt)`\n",
    "Return a dtype like the input, tagged to be stored as HDF5 opaque type.\n",
    "\n",
    "##### `h5py.check_opaque_dtype(dt)`\n",
    "Return True if the dtype given is tagged to be stored as HDF5 opaque data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Writer Multiple Reader\n",
    "Allows simple concurrent reading of a HDF5 file while it is being written from another process. \n",
    "\n",
    "The following basic steps are typically required by writer and reader processes:\n",
    "\n",
    "- Writer process creates the target file and all groups, datasets and attributes.\n",
    "- Writer process switches file into SWMR mode.\n",
    "- Reader process can open the file with `swmr=True`.\n",
    "- Writer writes and/or appends data to existing datasets (new groups and datasets cannot be created when in SWMR mode).\n",
    "- Writer regularly flushes the target dataset to make it visible to reader processes.\n",
    "- Reader refreshes target dataset before reading new meta-data and/or main data.\n",
    "- Writer eventually completes and close the file as normal.\n",
    "- Reader can finish and close file as normal whenever it is convenient.\n",
    "\n",
    "The following snippet demonstrate a SWMR writer appending to a single dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"swmr.h5\", 'w', libver='latest')\n",
    "arr = np.array([1,2,3,4])\n",
    "dset = f.create_dataset(\"data\", chunks=(2,), maxshape=(None,), data=arr)\n",
    "f.swmr_mode = True\n",
    "# Now it is safe for the reader to open the swmr.h5 file\n",
    "for i in range(5):\n",
    "    new_shape = ((i+1) * len(arr), )\n",
    "    dset.resize( new_shape )\n",
    "    dset[i*len(arr):] = arr\n",
    "    dset.flush()\n",
    "    # Notify the reader process that new data has been written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual datasets\n",
    "Virtual datasets allow a number of real datasets to be mapped together into a single, sliceable dataset via an interface layer. The mapping can be made ahead of time, before the parent files are written, and is transparent to the parent dataset characteristics (SWMR, chunking, compression etc…). The datasets can be meshed in arbitrary combinations, and even the data type converted.\n",
    "\n",
    "Once a virtual dataset has been created, it can be read just like any other HDF5 dataset.\n",
    "\n",
    "##### Warning\n",
    "Virtual dataset files cannot be opened with versions of the hdf5 library older than 1.10.\n",
    "\n",
    "To make a virtual dataset using h5py, you need to:\n",
    "\n",
    "- Create a VirtualLayout object representing the dimensions and data type of the virtual dataset.\n",
    "- Create a number of VirtualSource objects, representing the datasets the array will be built from. These objects can be created either from an h5py Dataset, or from a filename, dataset name and shape. This can be done even before the source file exists.\n",
    "- Map slices from the sources into the layout.\n",
    "- Convert the VirtualLayout object into a virtual dataset in an HDF5 file.\n",
    "\n",
    "The following snippet creates a virtual dataset to stack together four 1D datasets from separate files into a 2D dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = h5py.VirtualLayout(shape=(4, 100), dtype='i4')\n",
    "\n",
    "for n in range(1, 5):\n",
    "    filename = \"{}.h5\".format(n)\n",
    "    vsource = h5py.VirtualSource(filename, 'data', shape=(100,))\n",
    "    layout[n - 1] = vsource\n",
    "\n",
    "# Add virtual dataset to output file\n",
    "with h5py.File(\"VDS.h5\", 'w', libver='latest') as f:\n",
    "    f.create_virtual_dataset('data', layout, fillvalue=-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "Create a file. Create a hierarchy of groups and datasets with the following details:\n",
    "\n",
    "- This\n",
    "    - one\n",
    "        - dataset1 (1-100)\n",
    "    - two\n",
    "- That\n",
    "    - one\n",
    "        - dataset2 (2D)\n",
    "    - two\n",
    "    - three\n",
    "        - varlendataset (a variable length dataset) \n",
    "\n",
    "Print dataset2 using iteration.\n",
    "\n",
    "Create a minimum of 2 attributes for each dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
