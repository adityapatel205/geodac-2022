{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eddbbf7",
   "metadata": {},
   "source": [
    "## Day 1 Task: Reading hierarchical data files.\n",
    "\n",
    "This task is to familiarize you with reading and manipulating data from a file. When you obtain satellite data, it will usually come stored in a hierarchical data format, like NetCDF or HDF. These are self-describing data formats that contain not only multiple layers of data, but also the dimensions, attributes, units, and other information that goes with each data field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfa8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries used\n",
    "\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6261201",
   "metadata": {},
   "source": [
    "### Reading an HDF file from NASA Earthdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6574ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the File() function to open and create files\n",
    "\n",
    "viirs = h5py.File('VNP46A2.A2022265.h10v04.001.2022273135614.h5', \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b26b27",
   "metadata": {},
   "source": [
    "Here we provide an example file from the VIIRS instrument to demonstrate the functionality of the h5py library. Later on, you might download data on your own from NASA's EarthData or ESA's Copernicus portals.\n",
    "\n",
    "### Valid modes of this function are:\n",
    "\n",
    "'r' - For reading only, the file must exist (it is also the default).\n",
    "\n",
    "'r+' - For reading and writing, the file must exist.\n",
    "\n",
    "'w' - Used to create a file or truncate if it exists.\n",
    "\n",
    "'w'- or 'x' - For creating a file, the command fails if the file already exists.\n",
    "\n",
    "'a' - For reading and writing if it already exists, creates a file otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc8be6",
   "metadata": {},
   "source": [
    "### Getting items in the base directory\n",
    "\n",
    "An HDF file is hierarchical, the same way that the file system on your computer is. You can think of groups that contain data in an HDF file a little like a folder on your computer contains files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ccc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_items=list(viirs.items())\n",
    "print('Items in the base directory:',base_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2aa1fa",
   "metadata": {},
   "source": [
    "### Getting items from the subgroups\n",
    "\n",
    "Just like your filesystem, we can traverse the file to look at different parts of it. From the output of the previous command, we see two top-level items name \"HDFEOS\" and \"HDFEOS INFORMATION\". Let's have a closer look at one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading subgroups \n",
    "#here we are getting items from the 'HDFEOS' group\n",
    "g1=viirs.get('HDFEOS')\n",
    "g1_items=list(g1.items())\n",
    "print('Items in HDFEOS',g1_items)\n",
    "\n",
    "#Similarly we can get items of the 'HDFEOS INFORMATION' group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening a subgroup\n",
    "\n",
    "g2=g1.get('ADDITIONAL') #g1 has got the group HDFEOS and from g1 we can derive the subgroups of HDFEOS\n",
    "g2_items=list(g2.items())\n",
    "print('Items in subgroup ADDITIONAL',g2_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb217e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also open the subgroups using a full location path\n",
    "g3=viirs.get('/HDFEOS/GRIDS/VNP_Grid_DNB/Data Fields')\n",
    "g3_items=list(g3.items())\n",
    "print('Items in VNP_Grid_DNB',g3_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443223d",
   "metadata": {},
   "source": [
    "### Opening a dataset\n",
    "\n",
    "In the last output, instead of \"HDF5 group\", you start to see \"HDF5 dataset\". To work with the actual datasets in the file, we can use the objects captured in our last get command to print out the names of all these datasets. Then we can convert the dataset directly into a numpy array and treat it as an array in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a list of all the datasets that are available in the path /HDFEOS/GRIDS/VNP_Grid_DNB/Data Fields\n",
    "\n",
    "ls=list(g3.keys())\n",
    "print('List of datasets in Data Fields: \\n',ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3734924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are opening the dataset and converting it into a numpy array\n",
    "#Once it is in numpy array format you can perform array functions on it\n",
    "\n",
    "datavalues1=np.array(g3.get('DNB_BRDF-Corrected_NTL'))\n",
    "print('Shape of datavalues1: \\n', datavalues1.shape)\n",
    "print(datavalues1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55742669",
   "metadata": {},
   "source": [
    "### Read attributes of the dataset\n",
    "\n",
    "Remember that each data field also comes with all the information that describes it (HDF is a self-describing file format). Each dataset is created with a set of attributes that tell you things like what the units of the data are, or how a \"fill\" value is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=g3.get('DNB_BRDF-Corrected_NTL')\n",
    "k=list(dataset1.attrs.keys()) #list of keys\n",
    "v=list(dataset1.attrs.values()) #list of values\n",
    "print(k[:]) \n",
    "print(v[:])\n",
    "\n",
    "\n",
    "#first value should correspond to the first key\n",
    "\n",
    "print(dataset1.attrs[k[0]]) #here we are printing the value which corresponds to the first key\n",
    "\n",
    "print(k[4],v[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b1bef",
   "metadata": {},
   "source": [
    "## Try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ba940",
   "metadata": {},
   "source": [
    "#### 1. Open the example .h5 file, or download your own from search.earthdata.nasa.gov\n",
    "\n",
    "    The example file is from the dataset called \"VIIRS/NPP Gap-Filled Lunar BRDF-Adjusted Nighttime Lights Daily L3 Global 500m Linear Lat Lon Grid\"\n",
    "\n",
    "#### 2. Read the items in the base directory items into a list and print the list\n",
    "\n",
    "    Can you list all of the datasets in the file using a recursive function call?\n",
    "\n",
    "#### 3. Read in the data attributes to find the units and scale factor for the \"DNB_Lunar_Irradiance\" data\n",
    "\n",
    "    Can you determine the latitude and longitude range of the data in this file?\n",
    "\n",
    "#### 4. Read in data values to a numpy array and perform a calculation\n",
    "\n",
    "    What is the average value of the \"Gap_Filled_DNB_BRDF-Corrected_NTL\" data for this scene? Don't forget to remove the fill values!\n",
    "\n",
    "    Often we use one data field as a way to filter another, especially for things like cloud contamination. If you screen out all of the \"Gap_Filled_DNB_BRDF-Corrected_NTL\" data where the \"QF_Cloud_Mask\" is higher than 200, what is the average value now?\n",
    "\n",
    "#### 5. Try plotting the data values from \"Latest_High_Quality_Retrieval\" using the plot_surface() function of pyplot\n",
    "\n",
    "    You might need to create a grid using the latitude and longitude range and the number of points in your array.\n",
    "\n",
    "    What geographical features do you recognize in the data?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f0326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEODACenv",
   "language": "python",
   "name": "geodacenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
